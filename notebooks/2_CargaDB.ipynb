{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook #2: Carga: creación de Base de Datos SQL\n",
    "\n",
    "En este segundo notebook crearemos y cargaremos nuestra base de datos.\n",
    "\n",
    "- El primer paso será importar las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfranco/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Librerías para tratamiento de datos\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "pd.set_option('display.max_columns', None) # Parámetro que modifica la visualización de los DFs\n",
    "\n",
    "# Librería para el acceso a variables y funciones\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src import soporte_funciones as sf #Archivo .py donde encontraremos todas nuestras funciones.\n",
    "from src import soporte_variables as sv\n",
    "\n",
    "# Librería para trabajar con bases de datos SQL\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError, errorcodes, errors\n",
    "\n",
    "# Librería para ignorar avisos\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Ignora TODOS los avisos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora importaremos los CSVs creados en el notebook anterior, y crearemos listas de tuplas, que es el formato necesario para la carga a la base de datos. Los geojson los trataremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingresos_hogares = pd.read_csv(\"../datos/finales/ingresos_hogares.csv\", index_col=0)\n",
    "df_poblacion = pd.read_csv(\"../datos/finales/poblacion.csv\", index_col=0)\n",
    "df_redpiso = pd.read_csv(\"../datos/finales/redpiso.csv\", index_col=0)\n",
    "\n",
    "lista_ingresos = list(df_ingresos_hogares.itertuples(index=False, name=None))\n",
    "lista_poblacion = list(df_poblacion.itertuples(index=False, name=None))\n",
    "lista_redpiso = list(df_redpiso.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El tercer paso consiste en crear la base de datos, para lo cual, usaremos la función `dbeaver_crear_db()`, que recibe como único argumento el nombre de la base de datos que deseamos crear. En su código, se crea la conexión a DBeaver, utilizando los parámetros de conexión (usuario y contraseña) que se han guardado en el soporte `../src/.env` (oculto) e importado en  `../src/soporte_funciones.py`.\n",
    "\n",
    "- Llamaremos a la base de datos \"AlquileresMadrid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos AlquileresMadrid creada con éxito\n"
     ]
    }
   ],
   "source": [
    "#sf.dbeaver_crear_db(\"alquileresmadrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los datos se insertarán en las tablas que crearemos a continuación, con ayuda de las queries de creación definidas en `../src/soporte_variables.py`.\n",
    "\n",
    "- Para crear esas tablas, hemos decidido que nuestra base de datos se estructurará de la manera en que vemos en el siguiente esquema entidad relación, que muestra la información que contiene cada tabla y cómo se relaciona entre sí. En el README del proyecto se explica su diseño.\n",
    "\n",
    "<img src=\"../images/DiagramaER.png\" width=\"400\">\n",
    "\n",
    "\n",
    "- En este caso utilizaremos dos funciones:\n",
    "\n",
    "    - `sf.dbeaver_conexion()`: recibe como parámetro el nombre de la base de datos de DBeaver y crea la conexión entre el notebook y la base de datos.\n",
    "    - `sf.dbeaver_commit()`: recibe como parámetros la conexión a DBeaver y la query de creación, realizando el commit hacia la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit realizado\n",
      "Commit realizado\n",
      "Commit realizado\n",
      "Commit realizado\n",
      "Commit realizado\n",
      "Commit realizado\n"
     ]
    }
   ],
   "source": [
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_distritos)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_airbnb)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_idealista)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_redpiso)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_ingreso_hogar)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_poblacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con las tablas preparadas, continuamos con la inserción de los mismos. Primero insertaremos manualmente las tablas que contienen información geoespacial, por necesitar un tratamiento especial, y luego seguiremos con los csv tradicionales.\n",
    "- Empezaremos por los importar los archivos de tipo `geojson` que se han creado en el notebook #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_distritos = gpd.read_file(\"../datos/finales/distritos.geojson\")\n",
    "gdf_airbnb = gpd.read_file(\"../datos/finales/airbnb.geojson\")\n",
    "gdf_idealista = gpd.read_file(\"../datos/finales/idealista.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por orden de inserción, debemos empezar por las tablas que sólo contienen Primary Keys, en este caso, distritos.\n",
    "- Usaremos la función de conexión a DBeaver `sf.dbeaver_conexion()`, que recibe como argumento el nombre de la base de datos, así como las queries de inserción que hemos definido en `src/soporte_variables.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a base de datos y cursor\n",
    "conn = sf.dbeaver_conexion(\"alquileresmadrid\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Insertamos datos de cada GeoDataFrame:\n",
    "for _, row in gdf_distritos.iterrows():\n",
    "    id_distrito = row['ID_Distrito']\n",
    "    nombre = row['Distrito']\n",
    "    geom = row['geometry'].wkt\n",
    "\n",
    "    cur.execute(sv.query_inser_distritos, (id_distrito, nombre, geom))\n",
    "\n",
    "# Commit y cerrar\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para evitar errores cerramos el cursor y la conexión tras cada inserción. Seguimos con la tabla de AirBnB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a base de datos y cursor\n",
    "conn = sf.dbeaver_conexion(\"alquileresmadrid\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Insertamos datos de cada GeoDataFrame:\n",
    "for _, row in gdf_airbnb.iterrows():\n",
    "    id_distrito2 = row['ID_Distrito']\n",
    "    precio2 = row['Precio Total']\n",
    "    descripcion2 = row['Descripcion']\n",
    "    geom2 = row['geometry'].wkt\n",
    "    \n",
    "    cur.execute(sv.query_inser_airbnb, (id_distrito2, precio2, descripcion2, geom2))\n",
    "\n",
    "# Commit y cerrar\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente la tabla de Idealista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a base de datos y cursor\n",
    "conn = sf.dbeaver_conexion(\"alquileresmadrid\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Insertamos datos de cada GeoDataFrame:\n",
    "for _, row in gdf_idealista.iterrows():\n",
    "    id_distrito3 = row['ID_Distrito']\n",
    "    precio3 = row['Precio']\n",
    "    tipo3 = row['Precio']\n",
    "    planta3 = row['Planta']\n",
    "    tamanio3 = row['Tamanio']\n",
    "    habitaciones3 = row['Habitaciones']\n",
    "    banios3 = row['Banios']\n",
    "    direccion3 = row['Direccion']\n",
    "    descripcion3 = row['Descripcion']\n",
    "    geom3 = row['geometry'].wkt\n",
    "    \n",
    "    cur.execute(sv.query_inser_idealista, (id_distrito3, precio3, tipo3, planta3, tamanio3, habitaciones3, banios3, direccion3, descripcion3, geom3))\n",
    "\n",
    "# Commit y cerrar\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Insertados los datos en las tablas que contienen datos de geolocalización, continuamos con las tres restantes: ingresos por hogar, población y Redpiso.\n",
    "\n",
    "- Usaremos la función `sf.dbeaver_commitmany()`, que recibe como argumentos:\n",
    "    - la conexión a DBeaver (usando la función de conexión),\n",
    "    - las queries de inserción que hemos definido en `src/soporte_variables.py` y,\n",
    "    - los datos que deseamos insertar, en este caso, las listas de tuplas preparadas anteriormente.\n",
    "- Esta función envía múltiples datos desde el notebook hacia la base de datos DBeaver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit realizado\n"
     ]
    }
   ],
   "source": [
    "sf.dbeaver_commitmany(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_inser_ingreso_hogar,lista_ingresos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit realizado\n"
     ]
    }
   ],
   "source": [
    "sf.dbeaver_commitmany(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_inser_poblacion,lista_poblacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit realizado\n"
     ]
    }
   ],
   "source": [
    "sf.dbeaver_commitmany(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_inser_redpiso,lista_redpiso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con los datos insertados, podemos empezar a trabajar sobre nuestra base de datos. Continuamos con las consultas a la base de datos, la visualización y el análisis en el Notebook #3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
